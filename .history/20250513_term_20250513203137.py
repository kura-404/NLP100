from tqdm import tqdm
from datetime import datetime

# ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã¨ãƒªã‚¹ãƒˆä½œæˆ
print("ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
FILE_NAME = "/Users/kuramotomana/Test/æˆæœæ¦‚è¦.csv"
df = pd.read_csv(FILE_NAME, encoding='utf-8-sig')
research_abstracts = df['æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰'].dropna().drop_duplicates().tolist()

print(f"ğŸ”¢ ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ï¼ˆé‡è¤‡ãƒ»æ¬ æé™¤å¤–ï¼‰: {len(research_abstracts)}")

# ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²
CHUNK_SIZE = 200
chunks = [research_abstracts[i:i + CHUNK_SIZE] for i in range(0, len(research_abstracts), CHUNK_SIZE)]
total_batches = len(chunks)
print(f"ğŸ“¦ ãƒãƒ£ãƒ³ã‚¯æ•°: {total_batches}ï¼ˆ1ãƒãƒ£ãƒ³ã‚¯ â‰’ {CHUNK_SIZE}ä»¶ï¼‰")

# request_dataä½œæˆï¼†é€ä¿¡
print("ğŸš€ ãƒãƒƒãƒé€ä¿¡é–‹å§‹")
os.makedirs("data", exist_ok=True)
with tqdm(total=total_batches, desc="é€ä¿¡ä¸­", unit="ãƒãƒ£ãƒ³ã‚¯") as pbar:
    for i, chunk in enumerate(chunks):
        joined_chunk = "\n".join(chunk)
        request_data = [{
            "custom_id": f"request-{i}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "gpt-4.1-mini",
                "messages": [
                    {
                        "role": "system",
                        "content": "æ¬¡ã®ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ç”¨èªã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã¯æŠ½å‡ºã—ãŸç”¨èªã®ã¿ã€ç©ºç™½åŒºåˆ‡ã‚Šã§è¨˜è¿°ã—ã¦ãã ã•ã„ã€‚"
                    },
                    {
                        "role": "user",
                        "content": joined_chunk
                    }
                ],
                "max_tokens": 2000
            }
        }]
        jsonl_path = f"data/batch_{i}.jsonl"
        write_jsonl(jsonl_path, request_data)
        batch_input_file = client.files.create(file=open(jsonl_path, "rb"), purpose="batch")
        batch_metadata = client.batches.create(
            input_file_id=batch_input_file.id,
            endpoint="/v1/chat/completions",
            completion_window="24h",
            metadata={"description": "chunk-based-batch"}
        )
        with open('request_input_id.csv', 'a', newline='', encoding='utf-8') as f:
            writer = csv.writer(f)
            writer.writerow([jsonl_path, batch_metadata.id, batch_input_file.id])
        confirm_batch_status(batch_metadata, batch_input_file.id)
        pbar.update(1)

# å‡ºåŠ›å–å¾—
print("ğŸ“¥ å‡ºåŠ›å–å¾—ä¸­...")
df_request_batch = pd.read_csv("request_input_id.csv", header=None, names=["input_file", "batch_id", "input_file_id"])
batche_ids = df_request_batch['batch_id'].tolist()

yomi_outputs = []
with tqdm(total=len(batche_ids), desc="å–å¾—ä¸­", unit="ãƒãƒƒãƒ") as pbar:
    for batch_count, ids in enumerate(batche_ids):
        batches_information = client.batches.retrieve(ids)
        if batches_information.status == "completed" and batches_information.output_file_id is not None:
            file_response = client.files.content(batches_information.output_file_id)
            output = [json.loads(i) for i in file_response.text.strip().split('\n')]
            for o in output:
                yomi_output = o['response']['body']['choices'][0]['message']['content']
                yomi_outputs.append(yomi_output)
        pbar.update(1)

# CSVå‡ºåŠ›
print("ğŸ’¾ ç”¨èªãƒªã‚¹ãƒˆã‚’å‡ºåŠ›ä¸­...")
terms_list = []
for row in yomi_outputs:
    terms_list.extend(row.strip().split())

timestamp = datetime.now().strftime("%Y%m%d_%H%M")
base_name = os.path.splitext(os.path.basename(FILE_NAME))[0]
output_filename = f"{timestamp}_{base_name}_terms_only.csv"
with open(output_filename, 'w', encoding='utf-8', newline='') as f:
    f.write(','.join(terms_list))

print(f"âœ… å‡ºåŠ›å®Œäº†: {output_filename}ï¼ˆèªæ•°: {len(terms_list):,}ï¼‰")