#csvãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§8000ãƒˆãƒ¼ã‚¯ãƒ³ã”ã¨ã«1ãƒªã‚¹ãƒˆã«ã—ã¦ãƒªã‚¹ãƒˆãŒå¹¾ã¤ã§ãã‚‹ã‹ã‚’ç¢ºèªã™ã‚‹ã‚³ãƒ¼ãƒ‰
import csv
import tiktoken

csv_file_path="/Users/kuramotomana/Test/æˆæœæ¦‚è¦.csv"
target_column_name="æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"
max_tokens_per_list=8000

# === ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆæº–å‚™ï¼ˆcl100k_base = GPT-4o-miniç›¸å½“ï¼‰ ===
encoding = tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

# === ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã‚’æŠ½å‡º ===
unique_values = set()
with open(csv_file_path, newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        if target_column_name in row:
            value = row[target_column_name].strip()
            if value:
                unique_values.add(value)

# === ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§åˆ†å‰² ===
lists = []
current_list = []
current_tokens = 0

for value in sorted(unique_values):  # ä»»æ„ã«ã‚½ãƒ¼ãƒˆ
    tokens = count_tokens(value)
    if current_tokens + tokens > max_tokens_per_list:
        lists.append(current_list)
        current_list = [value]
        current_tokens = tokens
    else:
        current_list.append(value)
        current_tokens += tokens

# æœ€å¾Œã®ãƒªã‚¹ãƒˆã‚’è¿½åŠ 
if current_list:
    lists.append(current_list)

# === å‡ºåŠ› ===
print(f"âœ… ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã®æ•°: {len(unique_values)}")
print(f"âœ… å…¨ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {sum(count_tokens(v) for v in unique_values)}")
print(f"âœ… ãƒˆãƒ¼ã‚¯ãƒ³8000ä»¥å†…ã«åã¾ã‚‹ãƒªã‚¹ãƒˆæ•°: {len(lists)}")

# å„ãƒªã‚¹ãƒˆã®è©³ç´°
for i, lst in enumerate(lists):
    token_count = sum(count_tokens(v) for v in lst)
    print(f"  â–¶ List {i+1}: {len(lst)}é …ç›®, åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•° = {token_count}")

# ãƒªã‚¹ãƒˆã®æ•°ã‚’å¼·èª¿ã—ã¦å†è¡¨ç¤º
print(f"\nğŸ§¾ å…¨ä½“ã®ãƒªã‚¹ãƒˆæ•°ï¼ˆåˆ†å‰²ã•ã‚ŒãŸãƒªã‚¹ãƒˆã®å€‹æ•°ï¼‰: {len(lists)}")