import csv
import tiktoken
from datetime import datetime

# === è¨­å®š ===
# csv_file_path = "/Users/kuramotomana/Test/æˆæœæ¦‚è¦.csv"
# target_column_name = "æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"

csv_file_path='/Users/kuramotomana/Test/å…¨èª²é¡Œãƒ‡ãƒ¼ã‚¿æŠ½å‡º.csv'
target_column_name="ç ”ç©¶æ¦‚è¦"
max_tokens_per_list = 8000

# === ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆæº–å‚™ï¼ˆGPT-4o-miniç›¸å½“ï¼‰ ===
encoding = tiktoken.get_encoding("cl100k_base")

def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

# === ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã‚’æŠ½å‡º ===
unique_values = set()
with open(csv_file_path, newline='', encoding='utf-8') as csvfile:
    reader = csv.DictReader(csvfile)
    for row in reader:
        if target_column_name in row:
            value = row[target_column_name].strip()
            if value:
                unique_values.add(value)

# === ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ãƒªã‚¹ãƒˆåˆ†å‰² ===
lists = []
current_list = []
current_tokens = 0

for value in sorted(unique_values):
    tokens = count_tokens(value)
    if current_tokens + tokens > max_tokens_per_list:
        lists.append(current_list)
        current_list = [value]
        current_tokens = tokens
    else:
        current_list.append(value)
        current_tokens += tokens

if current_list:
    lists.append(current_list)

# === å‡ºåŠ›æº–å‚™ ===
output_lines = []
output_lines.append(f"âœ… ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªå€¤ã®æ•°: {len(unique_values)}")
output_lines.append(f"âœ… å…¨ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {sum(count_tokens(v) for v in unique_values)}")
output_lines.append(f"âœ… ãƒˆãƒ¼ã‚¯ãƒ³8000ä»¥å†…ã«åã¾ã‚‹ãƒªã‚¹ãƒˆæ•°: {len(lists)}\n")

for i, lst in enumerate(lists):
    token_count = sum(count_tokens(v) for v in lst)
    output_lines.append(f"  â–¶ List {i+1}: {len(lst)}é …ç›®, åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•° = {token_count}")

output_lines.append(f"\nğŸ§¾ å…¨ä½“ã®ãƒªã‚¹ãƒˆæ•°ï¼ˆåˆ†å‰²ã•ã‚ŒãŸãƒªã‚¹ãƒˆã®å€‹æ•°ï¼‰: {len(lists)}")

# === ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆï¼ˆç¾åœ¨æ™‚åˆ»ï¼‰===
timestamp = datetime.now().strftime("%Y%m%d_%H%M")
output_filename = f"/Users/kuramotomana/Test/{timestamp}_token_report.txt"

# === ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿ ===
with open(output_filename, "w", encoding="utf-8") as f:
    f.write("\n".join(output_lines))

# === çµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ ===
print(f"ğŸ“ çµæœã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {output_filename}")