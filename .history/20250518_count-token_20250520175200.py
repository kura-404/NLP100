import csv
import tiktoken
from datetime import datetime
import os
import pandas as pd

# === è¨­å®š ===
file_path = '/Users/kuramotomana/Test/å…¨èª²é¡Œãƒ‡ãƒ¼ã‚¿æŠ½å‡º.csv'  # .csv ã¾ãŸã¯ .xlsx
target_column_names = ["ç ”ç©¶æ¦‚è¦", "ç ”ç©¶ç›®çš„", "ç ”ç©¶æ–¹æ³•"]  # è¤‡æ•°åˆ—ã‚’ã“ã“ã«æŒ‡å®š
max_tokens_per_list = 8000

# === ãƒˆãƒ¼ã‚¯ãƒ³ã‚«ã‚¦ãƒ³ãƒˆæº–å‚™ï¼ˆGPT-4o-miniç›¸å½“ï¼‰ ===
encoding = tiktoken.get_encoding("cl100k_base")
def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

# === ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ï¼†çµåˆãƒ†ã‚­ã‚¹ãƒˆã®æŠ½å‡º ===
combined_values = set()
ext = os.path.splitext(file_path)[1].lower()

if ext == ".csv":
    with open(file_path, newline='', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            if all(col in row for col in target_column_names):
                parts = [row[col].strip() for col in target_column_names if row[col].strip()]
                if parts:
                    combined = " ".join(parts)
                    combined_values.add(combined)
elif ext in [".xlsx", ".xls"]:
    df = pd.read_excel(file_path)
    if all(col in df.columns for col in target_column_names):
        for _, row in df[target_column_names].dropna(how="all").iterrows():
            parts = [str(row[col]).strip() for col in target_column_names if pd.notnull(row[col]) and str(row[col]).strip()]
            if parts:
                combined = " ".join(parts)
                combined_values.add(combined)
else:
    raise ValueError("å¯¾å¿œã—ã¦ã„ãªã„ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã§ã™ã€‚CSVã‹Excelã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")

# === ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã§ãƒªã‚¹ãƒˆåˆ†å‰² ===
lists = []
current_list = []
current_tokens = 0

for value in sorted(combined_values):
    tokens = count_tokens(value)
    if current_tokens + tokens > max_tokens_per_list:
        lists.append(current_list)
        current_list = [value]
        current_tokens = tokens
    else:
        current_list.append(value)
        current_tokens += tokens
if current_list:
    lists.append(current_list)

# === å‡ºåŠ›æº–å‚™ ===
output_lines = []
output_lines.append(f"âœ… ãƒ¦ãƒ‹ãƒ¼ã‚¯ãªçµåˆãƒ†ã‚­ã‚¹ãƒˆæ•°: {len(combined_values)}")
output_lines.append(f"âœ… å…¨çµåˆãƒ†ã‚­ã‚¹ãƒˆã®åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•°: {sum(count_tokens(v) for v in combined_values)}")
output_lines.append(f"âœ… ãƒˆãƒ¼ã‚¯ãƒ³{max_tokens_per_list}ä»¥å†…ã«åã¾ã‚‹ãƒªã‚¹ãƒˆæ•°: {len(lists)}\n")

for i, lst in enumerate(lists):
    token_count = sum(count_tokens(v) for v in lst)
    output_lines.append(f"  â–¶ List {i+1}: {len(lst)}é …ç›®, åˆè¨ˆãƒˆãƒ¼ã‚¯ãƒ³æ•° = {token_count}")

output_lines.append(f"\nğŸ§¾ å…¨ä½“ã®ãƒªã‚¹ãƒˆæ•°ï¼ˆåˆ†å‰²ã•ã‚ŒãŸãƒªã‚¹ãƒˆã®å€‹æ•°ï¼‰: {len(lists)}")

# === å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«åä½œæˆ ===
base_name = os.path.splitext(os.path.basename(file_path))[0]
timestamp = datetime.now().strftime("%Y%m%d_%H%M")
output_filename = f"/Users/kuramotomana/Test/{timestamp}_{base_name}_token_report.txt"

# === æ›¸ãå‡ºã— ===
with open(output_filename, "w", encoding="utf-8") as f:
    f.write("\n".join(output_lines))

print(f"ğŸ“ çµæœã‚’å‡ºåŠ›ã—ã¾ã—ãŸ: {output_filename}")