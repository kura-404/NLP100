import subprocess
import sys

# å¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ç¢ºèª
def ensure_package_installed(package):
    try:
        __import__(package)
    except ImportError:
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])

# å¿…è¦ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ç¢ºèª
for pkg in ["tiktoken", "pandas", "openai", "scikit-learn"]:
    ensure_package_installed(pkg)

import pandas as pd
import tiktoken
import json
import random
import time
import difflib
from openai import OpenAI
from datetime import datetime
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI()

# ãƒˆãƒ¼ã‚¯ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
encoding = tiktoken.get_encoding("cl100k_base")
def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

# åŸ‹ã‚è¾¼ã¿å–å¾—
def get_embedding(text: str) -> list:
    response = client.embeddings.create(
        input=[text],
        model="text-embedding-3-small"
    )
    return response.data[0].embedding

# å„ãƒªã‚¯ã‚¨ã‚¹ãƒˆå‡¦ç†
def run_request(batch_num: int, request_num: int, examples_pool):
    # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã¨åˆæœŸåŒ–
    random.shuffle(examples_pool)
    example_text = ""
    total_tokens = 0
    used_count = 0
    examples_used = []

    for ex_id, ex_text in examples_pool:
        line = f"- {ex_text.strip()}\n"
        tokens = count_tokens(line)
        if total_tokens + tokens > 125000:
            break
        example_text += line
        total_tokens += tokens
        used_count += 1
        examples_used.append((ex_id, ex_text.strip()))

    # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
    prompt = f"""
ä»¥ä¸‹ã¯ç ”ç©¶æˆæœæ¦‚è¦ã®å®Ÿä¾‹ã§ã™ã€‚
ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€æ–°ãŸãªæ¶ç©ºã®æˆæœæ¦‚è¦ã‚’1ä»¶ã ã‘ä½œæˆã—ã¦ãã ã•ã„ã€‚

{example_text}
"""

    # ç”Ÿæˆ
    response = client.chat.completions.create(
        model="gpt-4.1-mini",
        messages=[{"role": "user", "content": prompt}],
        max_tokens=2000
    )
    generated = response.choices[0].message.content.strip()

    # é¡ä¼¼åº¦è¨ˆç®—
    emb_generated = get_embedding(generated)
    emb_examples = [get_embedding(text) for _, text in examples_used]
    similarities = cosine_similarity([emb_generated], emb_examples)[0]
    most_similar_index = int(np.argmax(similarities))
    most_similar_id, most_similar_text = examples_used[most_similar_index]
    similarity_score = similarities[most_similar_index]

    # å·®åˆ†æŠ½å‡º
    diff = list(difflib.ndiff(generated.split(), most_similar_text.split()))
    diff_text = "\n".join(diff)

    # ä¿å­˜
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    base_filename = f"{timestamp}_batch{batch_num}_req{request_num}_å‡ºåŠ›2000_å‚è€ƒ{used_count}ä»¶"

    with open(f"{base_filename}.txt", "w", encoding="utf-8") as f:
        f.write(generated)

    with open(f"{base_filename}_æ¯”è¼ƒçµæœ.txt", "w", encoding="utf-8") as f:
        f.write("ğŸ§ª ç”Ÿæˆã•ã‚ŒãŸæˆæœæ¦‚è¦\n")
        f.write(generated + "\n\n")
        f.write("ğŸ” æœ€ã‚‚é¡ä¼¼ã—ãŸå‚è€ƒæˆæœæ¦‚è¦\n")
        f.write(f"èª²é¡Œç®¡ç†ç•ªå·: {most_similar_id}\n")
        f.write(most_similar_text + "\n\n")
        f.write(f"ğŸ”— é¡ä¼¼åº¦ã‚¹ã‚³ã‚¢: {similarity_score:.4f}\n\n")
        f.write("ğŸ§¬ å·®åˆ†ï¼ˆå˜èªå˜ä½ï¼‰\n")
        f.write(diff_text)

    print(f"âœ… batch {batch_num} / request {request_num} å®Œäº†")

# ãƒãƒƒãƒå®Ÿè¡Œé–¢æ•°
def run_batches():
    FILE_PATH = "/Users/kuramotomana/Test/20250602_æœ€æ–°å¹´åº¦.xlsx"
    df = pd.read_excel(FILE_PATH)
    df_filtered = df[df["å¯¾è±¡ç–¾æ‚£"].astype(str).str.contains("æ–°ç”Ÿç‰©", na=False)]
    df_filtered = df_filtered.dropna(subset=["æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"])
    examples_pool = df_filtered[["èª²é¡Œç®¡ç†ç•ªå·", "æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"]].values.tolist()

    for batch_num in range(1, 11):
        for request_num in range(1, 11):
            run_request(batch_num, request_num, examples_pool)
            time.sleep(1)  # APIåˆ¶é™å¯¾ç­–ï¼ˆå¿…è¦ã«å¿œã˜ã¦èª¿æ•´ï¼‰

# å®Ÿè¡Œ
if __name__ == "__main__":
    run_batches()