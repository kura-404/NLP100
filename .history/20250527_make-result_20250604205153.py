import os
import numpy as np
import json
import time
import random
import tiktoken
import pandas as pd
from datetime import datetime
from openai import OpenAI
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm
import difflib

# OpenAI ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
client = OpenAI()

# ãƒˆãƒ¼ã‚¯ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
encoding = tiktoken.get_encoding("cl100k_base")
def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

# åŸ‹ã‚è¾¼ã¿å–å¾—é–¢æ•°ï¼ˆ8192ãƒˆãƒ¼ã‚¯ãƒ³ã¾ã§ï¼‰
def get_embedding(text: str, max_tokens: int = 8192) -> list:
    tokens = encoding.encode(text)
    if len(tokens) > max_tokens:
        tokens = tokens[:max_tokens]
    trimmed = encoding.decode(tokens)
    response = client.embeddings.create(
        input=[trimmed],
        model="text-embedding-3-small"
    )
    return response.data[0].embedding

# JSONLæ›¸ãå‡ºã—
def write_jsonl(filename, data):
    with open(filename, 'w', encoding='utf-8') as f:
        for entry in data:
            json.dump(entry, f, ensure_ascii=False)
            f.write('\n')

# ãƒãƒƒãƒé€²æ—ç¢ºèª
def wait_for_batch_completion(batch_id):
    print(f"ğŸ“¦ Batch {batch_id} submitted. Waiting for completion...")
    while True:
        status = client.batches.retrieve(batch_id).status
        if status == "completed":
            print(f"âœ… Batch {batch_id} completed.")
            return True
        elif status == "failed":
            print(f"âŒ Batch {batch_id} failed.")
            return False
        else:
            print(f"â³ Batch {batch_id} still {status}... Waiting 30s")
            time.sleep(30)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
FILE_PATH = "/Users/kuramotomana/Test/20250602_æœ€æ–°å¹´åº¦.xlsx"
df = pd.read_excel(FILE_PATH)
df_filtered = df[df["å¯¾è±¡ç–¾æ‚£"].astype(str).str.contains("æ–°ç”Ÿç‰©", na=False)]
df_filtered = df_filtered.dropna(subset=["æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰", "èª²é¡Œç®¡ç†ç•ªå·"])

# ãƒˆãƒ¼ã‚¯ãƒ³åˆ¶é™å†…ã§åŸ‹ã‚è¾¼ã¿å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿ã«é™å®š
examples_pool = []
for _, row in df_filtered.iterrows():
    text = str(row["æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"]).strip()
    if count_tokens(text) <= 8192:
        examples_pool.append((row["èª²é¡Œç®¡ç†ç•ªå·"], text))

# å‡ºåŠ›ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
os.makedirs("batch_jsonl", exist_ok=True)
os.makedirs("outputs", exist_ok=True)

# 100ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼ˆ10ãƒãƒƒãƒ Ã— 10ãƒªã‚¯ã‚¨ã‚¹ãƒˆï¼‰ã‚’ç”Ÿæˆ
all_requests = []
for batch_id in range(10):
    for req_id in range(10):
        random.shuffle(examples_pool)
        prompt_examples = ""
        total_tokens = 0
        used = []
        for ex_id, ex_text in examples_pool:
            line = f"- {ex_text}\n"
            tokens = count_tokens(line)
            if total_tokens + tokens > 125000:
                break
            prompt_examples += line
            total_tokens += tokens
            used.append((ex_id, ex_text))

        prompt = f"""
ä»¥ä¸‹ã¯ç ”ç©¶æˆæœæ¦‚è¦ã®å®Ÿä¾‹ã§ã™ã€‚
ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€æ–°ãŸãªæ¶ç©ºã®æˆæœæ¦‚è¦ã‚’1ä»¶ã ã‘ä½œæˆã—ã¦ãã ã•ã„ã€‚

{prompt_examples}
        """.strip()

        all_requests.append({
            "custom_id": f"batch{batch_id:02}_req{req_id:02}",
            "method": "POST",
            "url": "/v1/chat/completions",
            "body": {
                "model": "gpt-4.1-mini",
                "messages": [{"role": "user", "content": prompt}],
                "max_tokens": 2000
            }
        })

# ãƒãƒƒãƒé€ä¿¡ï¼‹å‡¦ç†ãƒ«ãƒ¼ãƒ—
for i in range(10):
    batch_requests = all_requests[i*10:(i+1)*10]
    jsonl_path = f"batch_jsonl/batch_{i:02}.jsonl"
    write_jsonl(jsonl_path, batch_requests)

    input_file = client.files.create(file=open(jsonl_path, "rb"), purpose="batch")
    batch = client.batches.create(
        input_file_id=input_file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h"
    )

    if wait_for_batch_completion(batch.id):
        batch_info = client.batches.retrieve(batch.id)
        if not batch_info.output_file_id:
            print(f"âš ï¸ Batch {batch.id} completed, but output_file_id is None. Skipping.")
            continue

        content_file = client.files.content(batch_info.output_file_id)
        lines = content_file.text.strip().split('\n')

        for line in lines:
            res = json.loads(line)
            req_id = res["custom_id"]
            gen_text = res['response']['body']['choices'][0]['message']['content'].strip()

            # é¡ä¼¼åº¦æ¤œç´¢
            gen_emb = get_embedding(gen_text)
            emb_refs = [get_embedding(txt) for _, txt in examples_pool]
            sims = cosine_similarity([gen_emb], emb_refs)[0]
            idx = int(np.argmax(sims))
            most_sim_id, most_sim_text = examples_pool[idx]
            diff_text = "\n".join(difflib.ndiff(gen_text.split(), most_sim_text.split()))

            # å‡ºåŠ›ä¿å­˜ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
            with open(f"outputs/{req_id}.txt", "w", encoding="utf-8") as f_txt:
                f_txt.write(gen_text)

            # æ¯”è¼ƒæƒ…å ±ä¿å­˜ï¼ˆCSVï¼‰
            df_out = pd.DataFrame([{
                "ãƒªã‚¯ã‚¨ã‚¹ãƒˆID": req_id,
                "ç”Ÿæˆã•ã‚ŒãŸæˆæœæ¦‚è¦": gen_text,
                "æœ€ã‚‚é¡ä¼¼ã—ãŸæˆæœæ¦‚è¦": most_sim_text,
                "èª²é¡Œç®¡ç†ç•ªå·": most_sim_id,
                "é¡ä¼¼åº¦": sims[idx],
                "å·®åˆ†": diff_text
            }])
            df_out.to_csv(f"outputs/{req_id}_æ¯”è¼ƒ.csv", index=False, encoding="utf-8-sig")
    else:
        print(f"âŒ Failed batch {i}")