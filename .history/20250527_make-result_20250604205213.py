import pandas as pd
import tiktoken
import json
import random
import time
import os
from datetime import datetime

# OpenAIã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
from openai import OpenAI
client = OpenAI()

# ãƒˆãƒ¼ã‚¯ãƒ³ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€
encoding = tiktoken.get_encoding("cl100k_base")
def count_tokens(text: str) -> int:
    return len(encoding.encode(text))

# è¨­å®š
MAX_INPUT_TOKENS = 125000
MAX_OUTPUT_TOKENS = 2000
FILE_PATH = "/Users/kuramotomana/Test/20250602_æœ€æ–°å¹´åº¦.xlsx"
TXT_OUT_DIR = "txt_outputs"
CSV_OUT_FILE = f"{datetime.now().strftime('%Y%m%d_%H%M')}_æˆæœæ¯”è¼ƒçµæœ.csv"
os.makedirs(TXT_OUT_DIR, exist_ok=True)

# ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã¨äº‹å‰ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
df = pd.read_excel(FILE_PATH)
df = df[df["å¯¾è±¡ç–¾æ‚£"].astype(str).str.contains("æ–°ç”Ÿç‰©", na=False)]
df = df.dropna(subset=["æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰", "èª²é¡Œç®¡ç†ç•ªå·"])
df["æˆæœãƒˆãƒ¼ã‚¯ãƒ³"] = df["æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"].apply(lambda x: count_tokens(str(x)))
df = df[df["æˆæœãƒˆãƒ¼ã‚¯ãƒ³"] <= 8192].copy()
df = df[["èª²é¡Œç®¡ç†ç•ªå·", "æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"]].reset_index(drop=True)

# ãƒãƒƒãƒä½œæˆ
def create_prompt(examples):
    body = "\n".join([f"- {ex}" for ex in examples])
    return f"""ä»¥ä¸‹ã¯ç ”ç©¶æˆæœæ¦‚è¦ã®å®Ÿä¾‹ã§ã™ã€‚
ã“ã‚Œã‚‰ã‚’å‚è€ƒã«ã€æ–°ãŸãªæ¶ç©ºã®æˆæœæ¦‚è¦ã‚’1ä»¶ã ã‘ä½œæˆã—ã¦ãã ã•ã„ã€‚

{body}
"""

def prepare_batch_requests(df, total_batches=10, requests_per_batch=10):
    all_requests = []
    for batch_idx in range(total_batches):
        for req_idx in range(requests_per_batch):
            seed_df = df.sample(frac=1).reset_index(drop=True)
            example_texts = []
            used = []
            token_sum = 0
            for i, row in seed_df.iterrows():
                example = str(row["æˆæœæ¦‚è¦ï¼ˆæ—¥æœ¬èªï¼‰"]).strip()
                tokens = count_tokens(f"- {example}\n")
                if token_sum + tokens > MAX_INPUT_TOKENS:
                    break
                token_sum += tokens
                example_texts.append(example)
                used.append((row["èª²é¡Œç®¡ç†ç•ªå·"], example))
            prompt = create_prompt(example_texts)
            custom_id = f"batch{batch_idx:02}_req{req_idx:02}"
            all_requests.append({
                "custom_id": custom_id,
                "method": "POST",
                "url": "/v1/chat/completions",
                "body": {
                    "model": "gpt-4.1-mini",
                    "messages": [{"role": "user", "content": prompt}],
                    "max_tokens": MAX_OUTPUT_TOKENS
                },
                "meta_examples": used  # ç‹¬è‡ªã‚­ãƒ¼ã§ä½¿ç”¨ä¾‹ã‚’ä¿å­˜
            })
    return all_requests

# JSONLã¨Metaä¿å­˜
def write_jsonl_and_meta(all_requests):
    jsonl_data = []
    meta_map = {}
    for req in all_requests:
        jsonl_data.append({
            "custom_id": req["custom_id"],
            "method": req["method"],
            "url": req["url"],
            "body": req["body"]
        })
        meta_map[req["custom_id"]] = req["meta_examples"]
    jsonl_path = "batch_requests.jsonl"
    with open(jsonl_path, "w", encoding="utf-8") as f:
        for line in jsonl_data:
            json.dump(line, f, ensure_ascii=False)
            f.write("\n")
    return jsonl_path, meta_map

# é¡ä¼¼åº¦ï¼†å·®åˆ†è¨ˆç®—
from sklearn.metrics.pairwise import cosine_similarity
import difflib
def get_embedding(text: str) -> list:
    tokens = encoding.encode(text)
    if len(tokens) > 8192:
        tokens = tokens[:8192]
    trimmed = encoding.decode(tokens)
    resp = client.embeddings.create(input=[trimmed], model="text-embedding-3-small")
    return resp.data[0].embedding

def compute_similarity_and_diff(generated: str, examples):
    emb_gen = get_embedding(generated)
    emb_refs = [get_embedding(text) for _, text in examples]
    sims = cosine_similarity([emb_gen], emb_refs)[0]
    idx = int(sims.argmax())
    sim_score = sims[idx]
    ref_id, ref_text = examples[idx]
    diff = list(difflib.ndiff(generated.split(), ref_text.split()))
    return ref_id, ref_text, sim_score, " ".join(diff)

# ãƒãƒƒãƒé€ä¿¡ã¨çµæœå‡¦ç†
def run_batch(jsonl_path, meta_map):
    input_file = client.files.create(file=open(jsonl_path, "rb"), purpose="batch")
    batch = client.batches.create(
        input_file_id=input_file.id,
        endpoint="/v1/chat/completions",
        completion_window="24h",
        metadata={"description": "æˆæœæ¦‚è¦ç”Ÿæˆãƒãƒƒãƒ"}
    )
    print(f"ğŸš€ Batch ID: {batch.id}")

    # å¾…æ©Ÿ
    while True:
        status = client.batches.retrieve(batch.id)
        if status.status == "completed":
            break
        elif status.status == "failed":
            raise RuntimeError(f"âŒ Batch {batch.id} failed.")
        time.sleep(60)

    # çµæœå–å¾—
    resp = client.files.content(status.output_file_id)
    outputs = [json.loads(line) for line in resp.text.strip().split("\n")]

    # å‡ºåŠ›å‡¦ç†
    results = []
    for entry in outputs:
        custom_id = entry["custom_id"]
        generated = entry["response"]["body"]["choices"][0]["message"]["content"].strip()
        examples = meta_map[custom_id]
        ref_id, ref_text, score, diff = compute_similarity_and_diff(generated, examples)

        # txtå‡ºåŠ›
        txt_path = os.path.join(TXT_OUT_DIR, f"{custom_id}_ç”Ÿæˆæˆæœ.txt")
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write(generated)

        # csvè¨˜éŒ²
        results.append([
            custom_id,
            generated,
            ref_id,
            ref_text,
            f"{score:.4f}",
            diff
        ])
    return results

# å®Ÿè¡Œæœ¬ä½“
def main():
    all_requests = prepare_batch_requests(df)
    jsonl_path, meta_map = write_jsonl_and_meta(all_requests)
    results = run_batch(jsonl_path, meta_map)

    # CSVå‡ºåŠ›
    df_out = pd.DataFrame(results, columns=[
        "custom_id", "ç”Ÿæˆæˆæœæ¦‚è¦", "é¡ä¼¼æˆæœ_èª²é¡Œç®¡ç†ç•ªå·", "é¡ä¼¼æˆæœ", "é¡ä¼¼åº¦", "å·®åˆ†"
    ])
    df_out.to_csv(CSV_OUT_FILE, index=False, encoding="utf-8-sig")
    print(f"âœ… æ¯”è¼ƒçµæœã‚’CSVã«ä¿å­˜ã—ã¾ã—ãŸ: {CSV_OUT_FILE}")

if __name__ == "__main__":
    main()