# ========= config.py =========
import os

# ãƒ¢ãƒ¼ãƒ‰: "test" ã¾ãŸã¯ "prod"
MODE = "test"

# ãƒ†ã‚¹ãƒˆç”¨è¨­å®š
TEST_FILE = "Test/LifeStory_2018_season1.xlsx"
TEST_COLUMN = "Sadness"
TEST_LIMIT = 100  # æœ€å¤§ãƒªã‚¯ã‚¨ã‚¹ãƒˆæ•°ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰

# æœ¬ç•ªç”¨è¨­å®š
INPUT_DIR = "excel_files"
BATCH_OUTPUT_DIR = "output/jsonl"

# å‡ºåŠ›è¨­å®š
os.makedirs(BATCH_OUTPUT_DIR, exist_ok=True)

# OpenAIãƒ¢ãƒ‡ãƒ«
MODEL = "gpt-4.1-mini"

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯ã“ã‚Œã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã«å–ã‚Šçµ„ã‚“ã§ãã ã•ã„ã€‚ä»¥ä¸‹ã®è¦ä»¶ã«å¾“ã£ã¦ã€å…¥åŠ›æ–‡ã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n\n"
    "ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã€‘\n"
    "â€“ ãƒ©ãƒ™ãƒ«: ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ï¼å®¶æ—/ãƒšãƒƒãƒˆ/å‹äºº/åŒåƒš/ãã‚Œä»¥å¤–ã®äººç‰©/ç™»å ´ãªã—\n"
    "â€“ è¤‡æ•°ãƒ©ãƒ™ãƒ«ã¯ç©ºç™½åŒºåˆ‡ã‚Šã§è¨˜è¿°ï¼ˆä¾‹ï¼šã€Œå®¶æ— å‹äººã€ï¼‰\n"
    "â€“ ãƒ©ãƒ™ãƒ«ãŒãªã‘ã‚Œã°ã€Œç™»å ´ãªã—ã€ã¨è¨˜è¿°"
)

# ========= test_mode.py =========
import pandas as pd
from openai import OpenAI
from tqdm import tqdm
from config import TEST_FILE, TEST_COLUMN, TEST_LIMIT, SYSTEM_PROMPT, MODEL

client = OpenAI()

def run_test_mode():
    df = pd.read_excel(TEST_FILE)
    if TEST_COLUMN not in df.columns:
        raise ValueError(f"åˆ— {TEST_COLUMN} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

    series = df[TEST_COLUMN].dropna().astype(str).iloc[:TEST_LIMIT]
    print(f"â–¶ï¸ ãƒ†ã‚¹ãƒˆãƒ¢ãƒ¼ãƒ‰å®Ÿè¡Œ: {len(series)}ã‚»ãƒ« (æœ€å¤§{TEST_LIMIT}ä»¶)")

    results = []
    for cell in tqdm(series, desc="åˆ†é¡ä¸­"):
        try:
            res = client.chat.completions.create(
                model=MODEL,
                messages=[
                    {"role": "system", "content": SYSTEM_PROMPT},
                    {"role": "user", "content": cell}
                ]
            )
            label = res.choices[0].message.content.strip()
        except Exception as e:
            label = "ã‚¨ãƒ©ãƒ¼"
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        results.append(label)

    out = pd.DataFrame({"å…ƒã®å€¤": series.tolist(), "åˆ†é¡ãƒ©ãƒ™ãƒ«": results})
    out.to_excel("output/test_result.xlsx", index=False)
    print("âœ… ãƒ†ã‚¹ãƒˆå‡ºåŠ›å®Œäº† â†’ output/test_result.xlsx")

# ========= batch_builder.py =========
import os
import json
import pandas as pd
from tqdm import tqdm
import tiktoken
from config import INPUT_DIR, BATCH_OUTPUT_DIR, SYSTEM_PROMPT, MODEL

encoding = tiktoken.encoding_for_model(MODEL)


def estimate_tokens(text):
    return len(encoding.encode(text))

def build_batches():
    files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".xlsx")]
    batch_id = 0

    for file in files:
        path = os.path.join(INPUT_DIR, file)
        df = pd.read_excel(path)
        records = []

        for col in df.columns:
            for cell in df[col].dropna().astype(str):
                user_content = cell.strip()
                tokens = estimate_tokens(SYSTEM_PROMPT) + estimate_tokens(user_content)

                records.append({
                    "messages": [
                        {"role": "system", "content": SYSTEM_PROMPT},
                        {"role": "user", "content": user_content}
                    ],
                    "tokens": tokens
                })

        total_tokens = sum(r["tokens"] for r in records)
        print(f"ğŸ“¦ {file}: {len(records)}ãƒªã‚¯ã‚¨ã‚¹ãƒˆ / æ¨å®š {total_tokens}ãƒˆãƒ¼ã‚¯ãƒ³")

        batch_path = os.path.join(BATCH_OUTPUT_DIR, f"batch_{batch_id}.jsonl")
        with open(batch_path, "w", encoding="utf-8") as f:
            for r in records:
                json.dump({"messages": r["messages"]}, f, ensure_ascii=False)
                f.write("\n")

        batch_id += 1

# ========= submit_batch.py =========
import os
from openai import OpenAI
from config import BATCH_OUTPUT_DIR, MODEL

client = OpenAI()

def submit_batches():
    files = sorted(f for f in os.listdir(BATCH_OUTPUT_DIR) if f.endswith(".jsonl"))
    for f in files:
        path = os.path.join(BATCH_OUTPUT_DIR, f)
        print(f"ğŸš€ é€ä¿¡: {f}")
        with open(path, "rb") as batch_file:
            resp = client.batches.create(
                input_file=batch_file,
                endpoint="/v1/chat/completions",
                completion_window="24h",
                model=MODEL
            )
            print(f"ğŸ†” Batch ID: {resp.id}")

# ========= main.py =========
from config import MODE
from test_mode import run_test_mode
from batch_builder import build_batches
from submit_batch import submit_batches

if MODE == "test":
    run_test_mode()
elif MODE == "prod":
    build_batches()
    submit_batches()
else:
    raise ValueError("MODE must be 'test' or 'prod'")