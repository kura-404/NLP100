# ========= ã‚ªãƒ¼ãƒ«ã‚¤ãƒ³ãƒ¯ãƒ³ç‰ˆã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆé€²æ—å‡ºåŠ›ä»˜ãï¼‰ =========
import os
import sys
import json
import time
import uuid
import pandas as pd
from tqdm import tqdm
from openai import OpenAI

# ========= è¨­å®š =========
MODE = "test"  # "test" ã¾ãŸã¯ "prod"

# ãƒ†ã‚¹ãƒˆç”¨
TEST_FILE = "Test/LifeStory_2018_season1.xlsx"
TEST_COLUMN = "Sadness"
TEST_LIMIT = 100

# æœ¬ç•ªç”¨
INPUT_DIR = "excel_files"
OUTPUT_DIR = "output/csv"
MODEL = "gpt-4.1-mini"
INTERVAL_SECONDS = 1
CHUNK_SIZE = 100

os.makedirs(OUTPUT_DIR, exist_ok=True)

SYSTEM_PROMPT = (
    "ã‚ãªãŸã¯ã“ã‚Œã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã«å–ã‚Šçµ„ã‚“ã§ãã ã•ã„ã€‚ä»¥ä¸‹ã®è¦ä»¶ã«å¾“ã£ã¦ã€å…¥åŠ›æ–‡ã‚’åˆ†é¡ã—ã¦ãã ã•ã„ã€‚\n\n"
    "ã€åˆ†é¡ã‚¿ã‚¹ã‚¯ã€‘\n"
    "â€“ ãƒ©ãƒ™ãƒ«: ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ï¼å®¶æ—ï¼ãƒšãƒƒãƒˆï¼å‹äººï¼åŒåƒšï¼ãã‚Œä»¥å¤–ã®äººç‰©ï¼AIï¼ç™»å ´ãªã—\n"
    "â€“ è¤‡æ•°ãƒ©ãƒ™ãƒ«ã¯ç©ºç™½åŒºåˆ‡ã‚Šã§è¨˜è¿°ï¼ˆä¾‹ï¼šã€Œå®¶æ— å‹äººã€ï¼‰\n"
    "â€“ ãƒ©ãƒ™ãƒ«ãŒãªã‘ã‚Œã°ã€Œç™»å ´ãªã—ã€ã¨è¨˜è¿°"
)

client = OpenAI()

def normalize_labels(label: str) -> str:
    return ",".join(label.replace("ã€€", " ").replace(",", " ").split())

def classify_chunk(chunk: list[str]) -> list[str]:
    prompt = "\n".join(chunk)
    try:
        res = client.chat.completions.create(
            model=MODEL,
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": prompt}
            ]
        )
        raw_output = res.choices[0].message.content.strip().split("\n")
        return [normalize_labels(label) for label in raw_output[:len(chunk)]]
    except Exception as e:
        return ["ã‚¨ãƒ©ãƒ¼"] * len(chunk)

def split_list(lst, n):
    return [lst[i:i + n] for i in range(0, len(lst), n)]

def process_file(filepath: str):
    df = pd.read_excel(filepath)
    file_id = os.path.splitext(os.path.basename(filepath))[0]

    print(f"ğŸ“„ ãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†é–‹å§‹: {file_id}")

    for col in df.columns:
        series = df[col].dropna().astype(str)
        if series.empty:
            continue

        print(f"ğŸ” å‡¦ç†ä¸­ã®åˆ—: {col} ({len(series)}è¡Œ)")
        chunks = split_list(series.tolist(), CHUNK_SIZE)
        results = []

        for i, chunk in enumerate(tqdm(chunks, desc=f"{file_id} - {col}")):
            print(f"  â–¶ï¸ ãƒãƒ£ãƒ³ã‚¯ {i + 1}/{len(chunks)} ä»¶æ•°: {len(chunk)}")
            labels = classify_chunk(chunk)
            results.extend(zip(chunk, labels))
            time.sleep(INTERVAL_SECONDS)

        df_out = pd.DataFrame(results, columns=["ã‚»ãƒ«ã®å†…å®¹", "åˆ†é¡ãƒ©ãƒ™ãƒ«"])
        output_path = os.path.join(OUTPUT_DIR, f"{file_id}-{col}.csv")
        df_out.to_csv(output_path, index=False, encoding="utf-8-sig")
        print(f"âœ… å‡ºåŠ›å®Œäº†: {output_path}\n")

def run_test_mode():
    df = pd.read_excel(TEST_FILE)
    if TEST_COLUMN not in df.columns:
        raise ValueError(f"åˆ— {TEST_COLUMN} ãŒå­˜åœ¨ã—ã¾ã›ã‚“")

    series = df[TEST_COLUMN].dropna().astype(str).iloc[:TEST_LIMIT]
    chunks = split_list(series.tolist(), CHUNK_SIZE)
    results = []

    print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆåˆ—: {TEST_COLUMN} ({len(series)}è¡Œ)")

    for i, chunk in enumerate(tqdm(chunks, desc="ãƒ†ã‚¹ãƒˆåˆ†é¡")):
        print(f"  â–¶ï¸ ãƒãƒ£ãƒ³ã‚¯ {i + 1}/{len(chunks)} ä»¶æ•°: {len(chunk)}")
        labels = classify_chunk(chunk)
        results.extend(zip(chunk, labels))
        time.sleep(INTERVAL_SECONDS)

    df_out = pd.DataFrame(results, columns=["ã‚»ãƒ«ã®å†…å®¹", "åˆ†é¡ãƒ©ãƒ™ãƒ«"])
    name = os.path.splitext(os.path.basename(TEST_FILE))[0]
    path = os.path.join(OUTPUT_DIR, f"{name}-{TEST_COLUMN}.csv")
    df_out.to_csv(path, index=False, encoding="utf-8-sig")
    print(f"âœ… ãƒ†ã‚¹ãƒˆå‡ºåŠ›å®Œäº†: {path}")

def run_prod_mode():
    files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".xlsx")]
    for file in files:
        process_file(os.path.join(INPUT_DIR, file))

if __name__ == "__main__":
    if MODE == "test":
        run_test_mode()
    elif MODE == "prod":
        run_prod_mode()
    else:
        raise ValueError("MODE must be 'test' ã¾ãŸã¯ 'prod'")
