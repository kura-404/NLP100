以下は、Zipfの法則に基づく日本語Wikipediaコーパスの頻度分析とグラフ描画コードについて、Python初心者向けに1行ずつ丁寧に解説した口頭用スクリプトです。

⸻

🔈 初心者向け口頭解説スクリプト

⸻

📘 まずはインポートから

import gzip

「gzip」は、拡張子 .gz で圧縮されたファイルを扱うための標準ライブラリです。

import json

「json」は、JSON形式のテキストを辞書やリストに変換するためのライブラリです。

import re

「re」は正規表現を扱うライブラリで、文字列の一部を抽出・置換するのに使います。

from collections import Counter

「Counter」は単語などの出現回数を自動で数えてくれる辞書型です。

import matplotlib.pyplot as plt

「matplotlib.pyplot」はグラフを描画するためのライブラリです。Colabでも利用できます。

import MeCab

「MeCab」は日本語の形態素解析器で、文章を単語に分けてその品詞も教えてくれます。

⸻

🧠 変数とデータの初期化

tagger = MeCab.Tagger()

ここではMeCabの形態素解析器を初期化しています。

word_counter = Counter()

単語の出現回数を記録する「カウンター」です。

file_path = "/content/drive/MyDrive/jawiki-country.json.gz"

Wikipediaの圧縮されたJSONファイルのパスを指定しています。ColabのGoogle Driveマウントを前提にしています。

⸻

📂 ファイルを読み込み、記事ごとに処理

with gzip.open(file_path, mode='rt', encoding='utf-8') as f:

gzipファイルをテキストモード（‘rt’）で開き、変数fで扱えるようにします。

    for line in f:

ファイルを1行ずつ読み込むループです。

        article = json.loads(line)

1行のJSON文字列を辞書（Pythonのdict）に変換します。

        text = article.get("text", "")

記事本文だけを取り出します。無ければ空文字になります。

        clean_text = re.sub(r'\[\[.*?\]\]', '', text)

Wiki記法のリンク「[[…]]」を除去しています。

        clean_text = re.sub(r"''+", '', clean_text)

同様に装飾記法（’’など）も除去しています。

⸻

🧪 MeCabで形態素解析して名詞だけカウント

        node = tagger.parseToNode(clean_text)

形態素解析のスタート地点を取得します。

        while node:

ノード（単語ごとの情報）をたどるループです。

            surface = node.surface

その単語の表層形（実際に文章に現れた形）を取得します。

            features = node.feature.split(',')

形態素情報をカンマ区切りで取得します。品詞などが入っています。

            pos = features[0]

最初の要素が品詞です。ここではそれを取得します。

            if pos.startswith("名詞"):

名詞であれば次の処理を行います。

                word_counter[surface] += 1

その単語の出現回数を1つ増やします。

            node = node.next

次の単語ノードに進みます。

⸻

📊 頻度を順位付きで整形

sorted_counts = word_counter.most_common()

出現回数の多い順に並べたリストを作成します。

ranks = list(range(1, len(sorted_counts) + 1))

順位を表すリストを作成します（1位、2位、3位…）。

frequencies = [freq for _, freq in sorted_counts]

単語の頻度だけをリストにまとめます。

⸻

📈 グラフ描画（日本語対応）

plt.rcParams['font.family'] = 'IPAexGothic'

グラフに日本語が使えるようにフォントを設定しています。

plt.figure(figsize=(10, 6))

グラフのサイズを指定します。

plt.plot(ranks, frequencies)

順位と頻度の組を線グラフとして描きます。

plt.xscale("log")
plt.yscale("log")

両軸を対数スケールにすることで、Zipfの法則に従う直線的な分布が見やすくなります。

plt.xlabel("出現頻度順位（対数）")
plt.ylabel("出現頻度（対数）")

軸ラベルを日本語で設定します。

plt.title("Zipfの法則に基づく単語頻度分布（日本語 Wikipedia）")

グラフのタイトルを設定します。

plt.grid(True)

グリッド（目盛りの補助線）を表示します。

plt.tight_layout()

レイアウトの崩れを防ぎます。

plt.show()

Colab上でグラフを表示します。

⸻

📌 変数一覧

変数名	説明
tagger	MeCabの形態素解析器インスタンス
word_counter	単語（表層形）の出現回数を記録する辞書
file_path	Wikipediaコーパスのファイルパス
article	各記事のJSONデータ（辞書形式）
text	記事本文
clean_text	Wiki記法などを除いたクリーンな本文
node	MeCabの解析結果をたどるノード
surface	表層形（実際の単語）
features	品詞などの形態素情報
pos	品詞情報
sorted_counts	頻度順に並べた単語と頻度のリスト
ranks	頻度順位のリスト
frequencies	出現頻度のリスト


⸻

もちろんです。以下に、Zipf（ジップ）の法則を、初心者にもわかりやすく丁寧に解説します。

⸻

🔍 Zipfの法則とは？

✔ 一言でいうと

「よく使われる単語はものすごく多く出現し、使われる回数が2番目、3番目…と減るにつれて急激に少なくなっていく」という自然言語の法則です。

⸻

🧠 もっと詳しく：Zipfの法則の中身

📊 頻度と順位の関係
	•	単語の出現頻度（どれだけ使われたか）と
	•	その順位（何番目に多く出た単語か）

を調べると、こんな関係があるとZipfさんが気づきました：

順位が2番目の単語の出現頻度は、1位の 約1/2
順位が3番目の単語の出現頻度は、1位の 約1/3
順位が10番目なら、1位の 約1/10

このように、頻度は順位の逆数に比例するのがZipfの法則です。

⸻

📐 数式で書くと

f(r) \propto \frac{1}{r}
	•	f(r)：r番目の単語の出現頻度
	•	r：順位（1位、2位、…）

「\propto」は「比例する」という意味です。

⸻

📉 実際にグラフにしてみると？

両対数グラフにすると…
	•	横軸：順位（出現回数が多い順）
	•	縦軸：その単語の出現頻度
	•	両方を対数スケールにする（log-logスケール）

➡ すると ほぼ直線になります。

これは、「単語の使われ方には規則性がある」ということを示しています。

⸻

🗣 どうしてこんな法則が成り立つの？

いろんな説がありますが、よく言われるのは：
	•	人間は使いやすい・分かりやすい単語をたくさん使う
	•	逆に難しい単語・専門的な語は限られた場面だけ使う

つまり、**言語には自然な“偏り”**があるんですね。

⸻

🧪 自然言語以外でもZipfの法則が見える例
	•	都市の人口：1位の都市の人口は2位の2倍、3位の3倍に近い
	•	ウェブサイトのアクセス数
	•	本の販売数（ベストセラーとロングテール）

実は、Zipfの法則は「自然言語」だけでなく、人間の活動の分布に広く見られる現象なのです。

⸻

✅ まとめ

項目	説明
Zipfの法則とは？	単語の頻度は順位の逆数に比例する
特徴	頻度の高い語が極端に多く、残りの語は少しずつ出現する
グラフで見ると	順位×頻度を両対数スケールにすると直線になる
応用例	自然言語、都市の人口、ウェブアクセスなど


⸻

もし、「Zipfの法則に従わないような文章ってあるの？」や、「プログラミングでどう扱えばいいの？」など疑問があれば、そこも詳しくお答えできます。お気軽にどうぞ。